{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import threading\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "FILE_PATTERN = r'*.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_files(files):\n",
    "    '''Shuffle loaded files'''\n",
    "    for file in files:\n",
    "        file_index = random.randint(0, (len(files) - 1))\n",
    "        yield files[file_index]\n",
    "\n",
    "\n",
    "def find_files(directory, pattern='*.wav'):\n",
    "    '''Recursively finds all files matching the pattern.'''\n",
    "    files = []\n",
    "    fnames = []\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(os.path.join(root, filename))\n",
    "            fnames.append(filename)\n",
    "    return files, fnames\n",
    "\n",
    "def get_category(fname):\n",
    "    '''Parse type from fnamepar'''\n",
    "    return fname.split('/')[-1].split('_')[0]\n",
    "\n",
    "def load_generic_audio(files, sample_rate, amount):\n",
    "    '''Generator that yields audio waveforms from the directory.'''\n",
    "    for it, filename in enumerate(files):\n",
    "        if it == amount:\n",
    "            break\n",
    "        category_id = get_category(filename)\n",
    "        audio, _ = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "        audio = audio.reshape(-1, 1)\n",
    "        yield audio, filename, category_id\n",
    "\n",
    "\n",
    "def trim_silence(audio, threshold, frame_length=512):\n",
    "    '''Removes silence at the beginning and end of a sample.'''\n",
    "    if audio.size < frame_length:\n",
    "        frame_length = audio.size\n",
    "    energy = librosa.feature.rmse(audio, frame_length=frame_length)\n",
    "    frames = np.nonzero(energy > threshold)\n",
    "    indices = librosa.core.frames_to_samples(frames)[1]\n",
    "\n",
    "    # Note: indices can be an empty array, if the whole audio was silence.\n",
    "    return audio[indices[0]:indices[-1]] if indices.size else audio[0:0]\n",
    "\n",
    "\n",
    "\n",
    "class AudioReader(object):\n",
    "    '''Generic background audio reader that preprocesses audio files\n",
    "    and add tham in lists'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 audio_dir,\n",
    "                 sample_rate,\n",
    "                 silence_threshold=None,\n",
    "                 sample_size=None,\n",
    "                 load_size=None):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_size = sample_size\n",
    "        self.silence_threshold = silence_threshold\n",
    "        self.load_size = load_size\n",
    "        self.counter = 0\n",
    "        self.time = time.time()\n",
    "\n",
    "        # TODO Find a better way to check this.\n",
    "        # Checking inside the AudioReader's thread makes it hard to terminate\n",
    "        # the execution of the script, so we do it in the constructor for now.\n",
    "\n",
    "        self.files, self.fnames = find_files(audio_dir)\n",
    "        if not self.files:\n",
    "            raise ValueError(\"No audio files found in '{}'.\".format(audio_dir))\n",
    "        self.pred_category = np.full(len(self.files), True)\n",
    "        if load_size is not None:\n",
    "            self.data = [0]*load_size\n",
    "            self.id = [0]*load_size\n",
    "        else:    \n",
    "            self.data = [0]*len(self.files)\n",
    "            self.id = [0]*len(self.files)\n",
    "        # Determine the number of mutually-exclusive categories we will\n",
    "        # accomodate in our embedding table.\n",
    "\n",
    "    def read(self):\n",
    "        #Read dataset\n",
    "        \n",
    "        iterator = load_generic_audio(self.files, self.sample_rate, self.load_size)\n",
    "        for audio, filename, category_id in iterator:\n",
    "            if self.silence_threshold is not None:\n",
    "                # Remove silence\n",
    "                audio = trim_silence(audio[:, 0], self.silence_threshold)\n",
    "                audio = audio.reshape(-1)\n",
    "                if audio.size == 0:\n",
    "                    self.pred_category[self.counter]=False\n",
    "\n",
    "\n",
    "            self.data[self.counter] = audio\n",
    "            self.id[self.counter] = category_id\n",
    "            self.counter += 1\n",
    "            if self.counter % 400 == 0:\n",
    "                print (time.time() - self.time,self.counter)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.02117681503296 400\n",
      "43.68047094345093 800\n",
      "65.40355324745178 1200\n",
      "89.51941466331482 1600\n",
      "113.04612159729004 2000\n",
      "134.56798648834229 2400\n",
      "154.92541027069092 2800\n",
      "175.22799634933472 3200\n",
      "196.10326719284058 3600\n",
      "216.31541323661804 4000\n",
      "236.01640033721924 4400\n",
      "258.4560263156891 4800\n",
      "280.1238281726837 5200\n",
      "301.908460855484 5600\n",
      "324.4076609611511 6000\n",
      "347.73490715026855 6400\n",
      "371.001829624176 6800\n",
      "393.31889939308167 7200\n",
      "415.85274863243103 7600\n",
      "436.7358467578888 8000\n",
      "459.60129475593567 8400\n",
      "480.13412261009216 8800\n",
      "500.8863036632538 9200\n",
      "520.6947932243347 9600\n",
      "544.0434896945953 10000\n",
      "563.1334483623505 10400\n",
      "583.7568407058716 10800\n",
      "605.5180835723877 11200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AudioReader at 0x7f037144ffd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dir = './data_v_7_stc/audio' \n",
    "sample_rate = 16000\n",
    "silence_threshold = 7e-4\n",
    "audio_reader = AudioReader(audio_dir,sr,silence_threshold=silence_threshold)\n",
    "audio_reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('./data_v_7_stc/meta/meta.txt',sep = '\\t', names=['file', 'q', 'w', 'e', 'label'])\n",
    "meta.drop(columns= ['q','w','e'],inplace=True)\n",
    "to = {'background':0,'bags':1,'door':2,'keyboard':3,'knocking_door':4,'ring':5,'speech':6,'tool':7}\n",
    "fr = {0:'background',1:'bags',2:'door',3:'keyboard',4:'knocking_door',5:'ring',6:'speech',7:'tool'}\n",
    "meta['label'] = meta['label'].map(to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "    return mfccs,chroma,mel,contrast,tonnetz\n",
    "\n",
    "def parse_audio_files(parent_dir,sub_dirs,file_ext='*.wav'):\n",
    "    features, labels = np.empty((0,193)), np.empty(0)\n",
    "    for label, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            mfccs, chroma, mel, contrast,tonnetz = extract_feature(fn)\n",
    "            ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n",
    "            features = np.vstack([features,ext_features])\n",
    "            labels = np.append(labels, fn.split('/')[2].split('-')[1])\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.017755508422851562\n",
      "500 14.90951132774353\n",
      "1000 29.10416316986084\n",
      "1500 43.933130979537964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rauf/anaconda3/lib/python3.6/site-packages/librosa/core/pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 58.297215700149536\n",
      "2500 73.46459031105042\n",
      "3000 87.80545735359192\n",
      "3500 104.1183774471283\n",
      "4000 118.8462381362915\n",
      "4500 132.37928175926208\n",
      "5000 149.08286380767822\n",
      "5500 164.07082200050354\n",
      "6000 180.5038948059082\n",
      "6500 195.36712551116943\n",
      "7000 210.31628489494324\n",
      "7500 227.9818766117096\n",
      "8000 242.1499993801117\n",
      "8500 257.6939375400543\n",
      "9000 271.8154282569885\n",
      "9500 287.50893211364746\n",
      "10000 304.1990976333618\n",
      "10500 318.4688255786896\n",
      "11000 334.30134630203247\n"
     ]
    }
   ],
   "source": [
    "n=len(audio_reader.data)\n",
    "features = np.zeros((n,187))\n",
    "start = time.time()\n",
    "for i,arr in enumerate(audio_reader.data):\n",
    "    if arr.size == 0:\n",
    "        continue\n",
    "    stft = np.abs(librosa.stft(arr))\n",
    "    mfcc=np.mean(librosa.feature.mfcc(arr,sr,n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(arr, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    features[i] = np.hstack([mfccs,chroma,contrast,mel])\n",
    "    if i%500 == 0:\n",
    "        print (i, time.time()-start)\n",
    "meta2 = meta.set_index('file')\n",
    "files = [0]*len(audio_reader.files)\n",
    "for i,f in enumerate(audio_reader.files):\n",
    "    files[i]=f.split('/')[-1]\n",
    "targets = np.array(meta2.loc[files]['label'])\n",
    "X_train, y_train = features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5809955596923828 400\n",
      "0 0.03862881660461426\n",
      "300 10.188040494918823\n",
      "600 19.95690655708313\n"
     ]
    }
   ],
   "source": [
    "test_dir='./data_v_7_stc/test'\n",
    "test_reader = AudioReader(test_dir,sr,silence_threshold=silence_threshold)\n",
    "test_reader.read()\n",
    "n=len(test_reader.data)\n",
    "X_full = np.zeros((n,187))\n",
    "start = time.time()\n",
    "for i,arr in enumerate(test_reader.data):\n",
    "    if arr.size == 0:\n",
    "        continue\n",
    "    stft = np.abs(librosa.stft(arr))\n",
    "    mfcc=np.mean(librosa.feature.mfcc(arr,sr,n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(arr, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    #tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(arr), sr=sr).T,axis=0)\n",
    "    X_full[i] = np.hstack([mfcc,chroma,contrast,mel])\n",
    "    if i%300 == 0:\n",
    "        print (i, time.time()-start)\n",
    "to = {'background':0,'bags':1,'door':2,'keyboard':3,'knocking':4,'ring':5,'speech':6,'tool':7}\n",
    "T = np.array(test_reader.id)\n",
    "X_test, y_test =X_full[T != 'unknown'],  np.array([to[x] for x in T[T != 'unknown']])\n",
    "X_unknown = X_full[T == 'unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9302325581395349"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb =  XGBClassifier(n_estimators=1000, max_depth=4, random_state=17)\n",
    "xgb.fit(X_train, y_train)\n",
    "pred = xgb.predict_proba(X_test)\n",
    "idx, score = pred.argmax(axis=1), pred.max(axis=1)\n",
    "accuracy_score(y_test,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['background_0077.wav' '0.9954573' 'background']\n",
      " ['bags_0003.wav' '0.9998622' 'bags']\n",
      " ['door_0023.wav' '0.99931216' 'door']\n",
      " ...\n",
      " ['background_t_0009.wav' '0.9921795' 'background']\n",
      " ['knocking_door_t_0028.wav' '0.6664679' 'background']\n",
      " ['bags_t_0014.wav' '0.99633235' 'bags']]\n"
     ]
    }
   ],
   "source": [
    "def write_pred(est, arr,files): \n",
    "    pred = est.predict_proba(arr)\n",
    "    idx, score = pred.argmax(axis=1), pred.max(axis=1)\n",
    "    lables = np.array([fr[x] for x in idx])\n",
    "    res = np.column_stack((files,score,lables))\n",
    "    print (res)\n",
    "    np.savetxt('to_ret.txt',res,delimiter='\\t',fmt=\"%s\")\n",
    "write_pred(xgb,X_full,test_reader.fnames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
